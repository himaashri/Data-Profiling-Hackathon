{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGSUHw0qM0Rp",
        "outputId": "6a96f272-d288-456e-c595-d9dc90950a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C20lLMTzS_Kl"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import uuid\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w1tA95PM8ya",
        "outputId": "84fba83f-fbdc-497a-d1bc-89be4fc00486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File not found at: /content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/mocked_financial_data.csv\n"
          ]
        }
      ],
      "source": [
        "# path to your CSV file\n",
        "file_path = '/content/drive/MyDrive/St/mocked_financial_data.csv'\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv(file_path)\n",
        "  # print(df.head())\n",
        "except FileNotFoundError:\n",
        "  print(f\"File not found at: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "kKJ9jXaPGimN",
        "outputId": "6ba3a6f9-dd12-4582-8abf-03ba852bd8f0"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e0teIOa0L-8"
      },
      "outputs": [],
      "source": [
        "unique_id = df.IdentifierValue\n",
        "df.drop(['IdentifierValue'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwAjwuE3SvuJ"
      },
      "outputs": [],
      "source": [
        "df=df.replace('missing',np.nan)\n",
        "df=df.replace('na',np.nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ySTnLuaTJhJ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0-Siyk7rgql"
      },
      "outputs": [],
      "source": [
        "# prompt: find date time features in df\n",
        "\n",
        "import pandas as pd\n",
        "# Find columns with datetime features\n",
        "date_features = []\n",
        "for col in df.columns:\n",
        "  if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
        "    date_features.append(col)\n",
        "\n",
        "print(\"Datetime columns found:\", datetime_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn83YvRXTOyD"
      },
      "outputs": [],
      "source": [
        "fig = df.isna().sum().sort_values().plot(kind = 'barh', figsize = (7, 12))\n",
        "plt.title('Percentage of Missing Values Per Column in Data Set', fontdict={'size':15})\n",
        "drop_cols=[]\n",
        "for p in fig.patches:\n",
        "    percentage ='{:,.0f}%'.format((p.get_width()/df.shape[0])*100)\n",
        "    width, height =p.get_width(),p.get_height()\n",
        "    x=p.get_x()+width+0.02\n",
        "    y=p.get_y()+height/2\n",
        "    fig.annotate(percentage,(x,y))\n",
        "    if (p.get_width()/df.shape[0])*100>90:\n",
        "      drop_cols.append(fig.get_yticklabels()[int(y)].get_text())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCQKyN4dXCNb"
      },
      "outputs": [],
      "source": [
        "# drop the columns with missing value percentage greater than 90% which are saves in drop_cols\n",
        "df=df.drop(drop_cols,axis=1)\n",
        "# save drop_cols to csv file\n",
        "pd.DataFrame(drop_cols).to_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/missing_value_columns.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cuQkN1HT0mO"
      },
      "outputs": [],
      "source": [
        "for feature in date_features:\n",
        "\n",
        "  df[feature].fillna(method='ffill', inplace=True)\n",
        "  # Convert to datetime objects\n",
        "  df[feature] = pd.to_datetime(df[feature])\n",
        "\n",
        "  # Extract features\n",
        "  df[feature + '_year'] = df[feature].dt.year\n",
        "  df[feature + '_month'] = df[feature].dt.month\n",
        "  df[feature + '_day'] = df[feature].dt.day\n",
        "  df[feature + '_dayofweek'] = df[feature].dt.dayofweek\n",
        "  df[feature + '_quarter'] = df[feature].dt.quarter\n",
        "  df[feature + '_is_weekend'] = (df[feature].dt.dayofweek // 5 == 1).astype(int)\n",
        "  df.drop([feature],axis=1,inplcae=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MswbPPTMUa59"
      },
      "outputs": [],
      "source": [
        "# check for variance\n",
        "numerical_features = [feature for feature in df.columns if df[feature].dtype!='O' and df[feature].dtype!='datetime64[ns]']\n",
        "variance = df[numerical_features].var(skipna=True)\n",
        "print(variance)\n",
        "# drop features with low variance\n",
        "low_variance_features = variance[variance < 0.001].index\n",
        "df = df.drop(low_variance_features, axis=1)\n",
        "#save features to csv file\n",
        "pd.DataFrame(low_variance_features).to_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/low_variance_features.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3ZVJ7f2VZWb"
      },
      "outputs": [],
      "source": [
        "# drop features with single unique value\n",
        "single_value_features = [feature for feature in df.columns if df[feature].nunique(dropna=True) == 1]\n",
        "df = df.drop(single_value_features, axis=1)\n",
        "# save dropped features to csv file\n",
        "pd.DataFrame(single_value_features).to_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/single_value_features.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8idhW1UWbG5"
      },
      "outputs": [],
      "source": [
        "# save all the dropped features in single csv file\n",
        "dropped_features = pd.concat([pd.DataFrame(low_variance_features), pd.DataFrame(single_value_features)], axis=0)\n",
        "# save the fdropped features in csv file\n",
        "dropped_features.to_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/dropped_features.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wwglixaXLbf"
      },
      "outputs": [],
      "source": [
        "# find all numerical features\n",
        "numerical_features = [feature for feature in df.columns if df[feature].dtype!='O' and df[feature].dtype!='datetime64[ns]']\n",
        "\n",
        "# calculate mean and median for each numerical feature\n",
        "mean_values = df[numerical_features].mean(skipna=True)\n",
        "median_values = df[numerical_features].median(skipna=True)\n",
        "\n",
        "# create a dataframe with the features, mean and median\n",
        "feature_stats = pd.DataFrame({'Feature': numerical_features, 'Mean': mean_values, 'Median': median_values})\n",
        "\n",
        "# write to csv file\n",
        "feature_stats.to_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/feature_stats.csv', index=False)\n",
        "\n",
        "# find all categorical features\n",
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# find the high frequency category for each categorical feature\n",
        "high_frequency_categories = df[categorical_features].apply(lambda x: x.mode()[0])\n",
        "\n",
        "# create a dataframe with features and their high fequency category\n",
        "high_frequency_df = pd.DataFrame({'Feature': high_frequency_categories.index, 'High Frequency Category': high_frequency_categories.values})\n",
        "\n",
        "# write to csv\n",
        "high_frequency_df.to_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/high_frequency_categories.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JB6GYZNbMh5"
      },
      "outputs": [],
      "source": [
        "[df[numerical_features].isnull().mean()>0][0].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3wUU3wVZcZR"
      },
      "outputs": [],
      "source": [
        "# load stats file\n",
        "stats = pd.read_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/feature_stats.csv')\n",
        "\n",
        "# create a dictionary for quick lookup\n",
        "mean_dict = stats.set_index('Feature')['Mean'].to_dict()\n",
        "median_dict = stats.set_index('Feature')['Median'].to_dict()\n",
        "\n",
        "# find numerical features with missing values\n",
        "numerical_features_with_missing_values = [df[numerical_features].isnull().mean()>0][0].index\n",
        "\n",
        "# fill all the missing values of numerical features based on outlier, in case of outlier fill with median, else mean\n",
        "for feature in numerical_features_with_missing_values:\n",
        "  if df[feature].isnull().sum()>0:\n",
        "    q1 = df[feature].quantile(0.25)\n",
        "    q3 = df[feature].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    has_outliers = (df[feature] < lower_bound) | (df[feature] > upper_bound)\n",
        "    if has_outliers.any():\n",
        "      df[feature].fillna(median_dict[feature], inplace=True)\n",
        "    else:\n",
        "      df[feature].fillna(mean_dict[feature], inplace=True)\n",
        "\n",
        "    df[feature].fillna(mean_dict[feature], inplace=True)\n",
        "\n",
        "#load the high frequency category data from the csv file\n",
        "high_frequency_categories = pd.read_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/high_frequency_categories.csv')\n",
        "\n",
        "# create dictionary for quick lookup\n",
        "high_frequency_dict = high_frequency_categories.set_index('Feature')['High Frequency Category'].to_dict()\n",
        "\n",
        "#find categorical features with missing values\n",
        "categorical_features_with_missing_values = [df[categorical_features].isnull().mean()>0][0].index\n",
        "\n",
        "# fill missing values of categorical features with high frequency category\n",
        "for feature in categorical_features_with_missing_values:\n",
        "  if feature in high_frequency_dict:\n",
        "    df[feature].fillna(high_frequency_dict[feature], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aezuo7pSdvky"
      },
      "outputs": [],
      "source": [
        "df.columns[df.isnull().sum()>0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fbQxA9vd874"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0xEZ_gxfO7K"
      },
      "outputs": [],
      "source": [
        "categorical_features = ['IdentifierType', 'AccountingIntent']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztsU4dUneaTS"
      },
      "outputs": [],
      "source": [
        "df[categorical_features].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8pG37owghQ3"
      },
      "outputs": [],
      "source": [
        "df['IdentifierType'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNSvLCG3gxVh"
      },
      "outputs": [],
      "source": [
        "df['AccountingIntent'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHb3PK9elgGt"
      },
      "outputs": [],
      "source": [
        "df[numerical_features].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xwdp4Fdeleq"
      },
      "outputs": [],
      "source": [
        "#encoding\n",
        "\n",
        "# write a fucntion for one hot encoding\n",
        "def one_hot_encode(df, columns):\n",
        "  for column in columns:\n",
        "    dummies = pd.get_dummies(df[column], prefix=column)\n",
        "    df = pd.concat([df, dummies], axis=1)\n",
        "    df = df.drop(column, axis=1)\n",
        "  return df\n",
        "\n",
        "# write a fucntion for frequency encoding\n",
        "def frequency_encode(df, columns):\n",
        "  for column in columns:\n",
        "    frequencies = df[column].value_counts(normalize=True)/(len(df)*0.01)\n",
        "    df[column + '_freq'] = df[column].map(frequencies)\n",
        "    df = df.drop(column, axis=1)\n",
        "  return df\n",
        "\n",
        "#write a function to rank categories based on frequency, highest frequency get 1, followed by 2,3,4\n",
        "def frequency_ranking_encode(df, columns):\n",
        "  for column in columns:\n",
        "    frequencies = df[column].value_counts(normalize=True)\n",
        "    frequencies={category: rank+1 for rank, category in enumerate(frequencies.index)}\n",
        "    df[column + '_freq'] = df[column].map(frequencies)\n",
        "    df = df.drop(column, axis=1)\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6EKAFqzlZIJ"
      },
      "outputs": [],
      "source": [
        "# find all numerical features\n",
        "numerical_features = [feature for feature in df.columns if df[feature].dtype!='O' and df[feature].dtype!='datetime64[ns]']\n",
        "\n",
        "# find discrete numerical features with nunique <10\n",
        "discrete_numerical_features = [feature for feature in numerical_features if df[feature].nunique() < 10]\n",
        "\n",
        "# find continuous numerical features\n",
        "continuous_numerical_features = [feature for feature in numerical_features if feature not in discrete_numerical_features]\n",
        "\n",
        "# convert discreate numerical features into string\n",
        "df[discrete_numerical_features] = df[discrete_numerical_features].astype(str)\n",
        "\n",
        "# find all categorical features\n",
        "categorical_features = [feature for feature in df.columns if df[feature].dtype=='O']\n",
        "\n",
        "df = frequency_ranking_encode(df, categorical_features)\n",
        "\n",
        "# find all numerical features -updated\n",
        "numerical_features = [feature for feature in df.columns if df[feature].dtype!='O' and df[feature].dtype!='datetime64[ns]']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vgBD7OanCTK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LKOTZosn3IY"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTPAC1pNoT_q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Find highly correlated columns (e.g., correlation > 0.9)\n",
        "highly_correlated_features = set()\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "  for j in range(i):\n",
        "    if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
        "      colname_i = correlation_matrix.columns[i]\n",
        "      colname_j = correlation_matrix.columns[j]\n",
        "      highly_correlated_features.add((colname_i, colname_j))\n",
        "\n",
        "print(\"Highly correlated features:\")\n",
        "for feature_pair in highly_correlated_features:\n",
        "  print(feature_pair)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq5wunbft1Kt"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hk_kDY9zBEG"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF_hmJ5mucgR"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=0.9)\n",
        "df_pca = pd.DataFrame(pca.fit_transform(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFQAPYrwuwxs"
      },
      "outputs": [],
      "source": [
        "# calculate feature contribution for each principal component\n",
        "feature_contributions = pd.DataFrame(pca.components_,columns=df.columns)\n",
        "# plot feature contribution\n",
        "for i in range(len(feature_contributions)):\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.bar(feature_contributions.columns,feature_contributions.iloc[i])\n",
        "  plt.title('Feature Contribution for Principal Component {}'.format(i+1))\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxxP6HJhvLmk"
      },
      "outputs": [],
      "source": [
        "# calculate explained variance and plot scree plot\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,len(explained_variance)+1),explained_variance,'o-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY-6IIxVv0Cp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "def plot_clustering(df,labels,title):\n",
        "  fig = plt.figure(figsize=(8,6))\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  unique_labels = np.unique(labels)\n",
        "  for label in unique_labels:\n",
        "    cluster_data = df[labels == label]\n",
        "    ax.scatter(cluster_data.iloc[:,0],cluster_data.iloc[:,1],cluster_data.iloc[:,2],label=label)\n",
        "  ax.set_title(title)\n",
        "  ax.set_xlabel('PC1')\n",
        "  ax.set_ylabel('PC2')\n",
        "  ax.set_zlabel('PC3')\n",
        "  ax.legend()\n",
        "  plt.show()\n",
        "\n",
        "def calculate_silhouette_scores(df,labels):\n",
        "  unique_labels = np.unique(labels)\n",
        "  avg_silhouette_scores = {}\n",
        "  if len(unique_labels)>1:\n",
        "    silhouette_scores = silhouette_samples(df,labels)\n",
        "    cluster_silhouette_scores={}\n",
        "    for label in unique_labels:\n",
        "      cluster_indices = np.where(labels == label)[0]\n",
        "      cluster_silhouette_scores[label] = silhouette_scores[cluster_indices]\n",
        "    for label,score in cluster_silhouette_scores.items():\n",
        "      avg_score = score.mean()\n",
        "      print(f\"Cluster {label}: Average Silhouette Score = {avg_score:.2f}\")\n",
        "      avg_silhouette_scores[label] = avg_score\n",
        "      print(score)\n",
        "      print()\n",
        "\n",
        "    print(\"the average silhouette score is\",silhouette_score(df,labels))\n",
        "    return avg_silhouette_scores\n",
        "  else:\n",
        "    print(\"No clusters found\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3-bO4lRxArQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# eps_values = np.arange(1,10,1)\n",
        "# silhouette_scores = []\n",
        "# for eps in eps_values:\n",
        "#   dbscan = DBSCAN(eps=eps,min_samples=5)\n",
        "#   labels = dbscan.fit_predict(df_pca)\n",
        "#   unique_labels = np.unique(labels)\n",
        "#   if len(unique_labels)>1:\n",
        "#     silhouette_scores.append(silhouette_score(df_pca,labels))\n",
        "#   else:\n",
        "#     silhouette_scores.append(0)\n",
        "# best_eps = eps_values[np.argmax(silhouette_scores)]\n",
        "# print(\"Best eps:\",best_eps)\n",
        "\n",
        "best_eps= 8\n",
        "\n",
        "DBSCAN = DBSCAN(eps=best_eps,min_samples=5)\n",
        "labels = DBSCAN.fit_predict(df_pca)\n",
        "plot_clustering(df_pca,labels,\"DBSCAN Clustering\")\n",
        "average_silhoutte_scores=calculate_silhouette_scores(df_pca,labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPsKi5Cy3FMz"
      },
      "outputs": [],
      "source": [
        "average_silhoutte_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z95zPjG0UsZ"
      },
      "outputs": [],
      "source": [
        "# prompt: add unique_id series to the dbscan labels\n",
        "\n",
        "import pandas as pd\n",
        "# Assuming 'labels' is the output from your DBSCAN clustering\n",
        "labels_with_unique_id = pd.DataFrame({'unique_id': unique_id, 'cluster_label': labels})\n",
        "\n",
        "# Now 'labels_with_unique_id' contains both the original unique identifier and the cluster label assigned by DBSCAN\n",
        "print(labels_with_unique_id.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeGzIzKN0f9R"
      },
      "outputs": [],
      "source": [
        "labels_with_unique_id.cluster_label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGC9V_Sy3NLR"
      },
      "outputs": [],
      "source": [
        "if average_silhoutte_scores:\n",
        "  cluster_labels_below_threshold = [\n",
        "      cluster_label for cluster_label, avg_score in average_silhoutte_scores.items()\n",
        "      if avg_score < 0.7\n",
        "  ]\n",
        "\n",
        "  # Filter the DataFrame to include only rows with cluster labels below the threshold\n",
        "  filtered_df = labels_with_unique_id[\n",
        "      labels_with_unique_id.cluster_label.isin(cluster_labels_below_threshold)\n",
        "  ]\n",
        "\n",
        "  print(\n",
        "      \"Cluster labels with average silhouette score less than 0.7:\",\n",
        "      filtered_df.cluster_label.unique(),\n",
        "  )\n",
        "  print(filtered_df)\n",
        "else:\n",
        "  print(\"No clusters found or silhouette scores not calculated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku3G_1CU3gRj"
      },
      "outputs": [],
      "source": [
        "df_copy = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5AM6edm3m9d"
      },
      "outputs": [],
      "source": [
        "# Get the unique identifiers from the filtered DataFrame\n",
        "identifier_values_to_retrieve = filtered_df['unique_id'].tolist()\n",
        "\n",
        "# Use these identifiers to retrieve datapoints from df_copy\n",
        "retrieved_datapoints = df_copy[df_copy['IdentifierValue'].isin(identifier_values_to_retrieve)]\n",
        "\n",
        "retrieved_datapoints\n",
        "\n",
        "# write to csv\n",
        "retrieved_datapoints.to_csv('/content/drive/MyDrive/CodeCrafters_WF/Model Input Data/Data/retrieved_datapoints.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
